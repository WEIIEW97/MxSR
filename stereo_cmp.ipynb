{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mono.mono_head import InferDAM\n",
    "from stereo.stereo_head import InferCREStereo\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/depth'\n",
    "# raft_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/raft'\n",
    "# gms_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/gmstereo'\n",
    "# cres_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/crestero'\n",
    "# cmp_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/research'\n",
    "# left_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/infra1'\n",
    "# right_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/infra2'\n",
    "# mono_dir='/home/william/extdisk/data/realsense-D455_depth_image/sample/damv2'\n",
    "# focal = 390.81134033203125\n",
    "# baseline = 94.994\n",
    "# cx = 320.83\n",
    "# cy = 245.31\n",
    "# # os.makedirs(cmp_dir, exist_ok=True)\n",
    "# # os.makedirs(mono_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_gts = sorted(Path('/home/william/extdisk/data/realsense-D455_depth_image/sample/depth').glob('*.png'))\n",
    "# f_rafts = sorted(Path(\"/home/william/extdisk/data/realsense-D455_depth_image/sample/raft\").glob('*.npy'))\n",
    "# f_gms = sorted(Path(\"/home/william/extdisk/data/realsense-D455_depth_image/sample/gmstereo\").glob('*_disp.pfm'))\n",
    "# f_cres = sorted(Path(\"/home/william/extdisk/data/realsense-D455_depth_image/sample/crestereo\").glob('*.npy'))\n",
    "# f_left = sorted(Path(left_dir).glob('*.png'))\n",
    "# f_right = sorted(Path(right_dir).glob('*.png'))\n",
    "# f_mono = sorted(Path(mono_dir).glob('*.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_pfm(file):\n",
    "    file = open(file, 'rb')\n",
    "\n",
    "    color = None\n",
    "    width = None\n",
    "    height = None\n",
    "    scale = None\n",
    "    endian = None\n",
    "\n",
    "    header = file.readline().rstrip()\n",
    "    if header.decode(\"ascii\") == 'PF':\n",
    "        color = True\n",
    "    elif header.decode(\"ascii\") == 'Pf':\n",
    "        color = False\n",
    "    else:\n",
    "        raise Exception('Not a PFM file.')\n",
    "\n",
    "    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode(\"ascii\"))\n",
    "    if dim_match:\n",
    "        width, height = list(map(int, dim_match.groups()))\n",
    "    else:\n",
    "        raise Exception('Malformed PFM header.')\n",
    "\n",
    "    scale = float(file.readline().decode(\"ascii\").rstrip())\n",
    "    if scale < 0:  # little-endian\n",
    "        endian = '<'\n",
    "        scale = -scale\n",
    "    else:\n",
    "        endian = '>'  # big-endian\n",
    "\n",
    "    data = np.fromfile(file, endian + 'f')\n",
    "    shape = (height, width, 3) if color else (height, width)\n",
    "\n",
    "    data = np.reshape(data, shape)\n",
    "    data = np.flipud(data)\n",
    "    return data*scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mono.depth_anything_v2.dinov2 import DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class DinoV2FeatureExtractor(nn.Module):\n",
    "    def __init__(self, dino_model):\n",
    "        super().__init__()\n",
    "        self.dino_model = dino_model\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        f = self.dino_model.get_intermediate_layers(x, n=1)[0]  \n",
    "        b, n, d = f.shape\n",
    "        # Remove CLS token\n",
    "        p = f[:, 1:, :]  \n",
    "        num_patches = p.shape[1]  # N - 1\n",
    "        s = int(math.sqrt(num_patches))\n",
    "        if s * s != num_patches:\n",
    "            raise ValueError(\n",
    "                f\"Number of patch tokens ({num_patches}) is not a perfect square.\"\n",
    "                f\" Image size or patch size might be incompatible.\"\n",
    "            )\n",
    "        # s = int(math.sqrt(n - 1))\n",
    "        p = p.reshape(b, s, s, d).permute(0, 3, 1, 2)\n",
    "        return p\n",
    "\n",
    "class DisparityRefinementNet(nn.Module):\n",
    "    def __init__(self, in_channels=2, feat_channels=384, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + feat_channels, 64, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, out_channels, 3, 1, 1)\n",
    "        )\n",
    "    def forward(self, dm, ds, dino_feat):\n",
    "        b, _, h, w = dm.shape\n",
    "        c = torch.cat([dm, ds], dim=1)\n",
    "        u = F.interpolate(dino_feat, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([c, u], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "def warp_right_to_left(right_img, disp):\n",
    "    b, c, h, w = right_img.shape\n",
    "    y = torch.linspace(0, h - 1, h, device=right_img.device).view(1, h, 1).expand(b, h, w)\n",
    "    x = torch.linspace(0, w - 1, w, device=right_img.device).view(1, 1, w).expand(b, h, w)\n",
    "    x_warp = x - disp[:, 0, :, :]\n",
    "    grid_x = 2.0 * x_warp / (w - 1) - 1.0\n",
    "    grid_y = 2.0 * y / (h - 1) - 1.0\n",
    "    grid = torch.stack([grid_x, grid_y], dim=3)\n",
    "    return F.grid_sample(right_img, grid, mode='bilinear', padding_mode='border', align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "import ast\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "class Middlebury2014(Dataset):\n",
    "    def __init__(self, data_list_path):\n",
    "        with open(data_list_path, \"r\") as f:\n",
    "            self.data_list = [line.strip() for line in f if line.strip()]\n",
    "        self.data_list.sort()\n",
    "        self.l_name = \"im0.png\"\n",
    "        self.r_name = \"im1.png\"\n",
    "        self.l_disp_name = \"disp0.pfm\"\n",
    "        self.r_disp_name = \"disp1.pfm\"\n",
    "\n",
    "        self.img_h = 518\n",
    "        self.img_w = 518\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_h, self.img_w)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene_dir = self.data_list[index]\n",
    "        left_img_path = os.path.join(scene_dir, self.l_name)\n",
    "        right_img_path = os.path.join(scene_dir, self.r_name)\n",
    "        left_disp_path = os.path.join(scene_dir, self.l_disp_name)\n",
    "        right_disp_path = os.path.join(scene_dir, self.r_disp_name)\n",
    "\n",
    "        left_img_bgr = cv2.imread(left_img_path, cv2.IMREAD_ANYCOLOR)\n",
    "        right_img_bgr = cv2.imread(right_img_path, cv2.IMREAD_ANYCOLOR)\n",
    "\n",
    "        left_img_rgb = cv2.cvtColor(left_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        right_img_rgb = cv2.cvtColor(right_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        left_disp = read_pfm(left_disp_path)\n",
    "        right_disp = read_pfm(right_disp_path)\n",
    "\n",
    "        left_img_pil = Image.fromarray(left_img_rgb)\n",
    "        right_img_pil = Image.fromarray(right_img_rgb)\n",
    "\n",
    "        left_img_tensor = self.transform(left_img_pil)\n",
    "        right_img_tensor = self.transform(right_img_pil)\n",
    "\n",
    "        left_disp_tensor = torch.from_numpy(left_disp).unsqueeze(0).float()\n",
    "        right_disp_tensor = torch.from_numpy(right_disp).unsqueeze(0).float()\n",
    "\n",
    "        sample = {\n",
    "            \"left_image\": left_img_tensor,   # (3, 518, 518)\n",
    "            \"right_image\": right_img_tensor, # (3, 518, 518)\n",
    "            \"left_disp\": left_disp_tensor,   # (1, H, W)\n",
    "            \"right_disp\": right_disp_tensor  # (1, H, W)\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "middlebury_data_list_path=\"/home/william/extdisk/data/middlebury/middlebury2014/middlebury2014_dataset.txt\"\n",
    "m2014_dataset = Middlebury2014(middlebury_data_list_path)\n",
    "dino_model = DINOv2('vits')\n",
    "mono_model = InferDAM()\n",
    "mono_model.initialize(model_path='/home/william/extdisk/checkpoints/depth-anything/depth_anything_v2_vits.pth', encoder='vits')\n",
    "\n",
    "stereo_model = InferCREStereo()\n",
    "stereo_model.initialize(model_path='/home/william/extdisk/checkpoints/CREStereo/crestereo_eth3d.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dino_extractor, mono_net, stereo_net, refine_net, dataloader, optimizer, alpha=0.1, device=\"cuda\"):\n",
    "    dino_extractor.eval()\n",
    "    refine_net.train()\n",
    "    s = 0\n",
    "    for batch in dataloader:\n",
    "        l = batch[\"left_image\"].to(device)\n",
    "        r = batch[\"right_image\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            print(f\"l shape is {l.shape}\")\n",
    "            fl = dino_extractor(l)\n",
    "            dm = mono_net.predict(l)\n",
    "            ds = stereo_net.predict(l, r)\n",
    "            gt = ds\n",
    "        pred = refine_net(dm, ds, fl)\n",
    "        loss_gt = F.l1_loss(pred, gt)\n",
    "        warped_r = warp_right_to_left(r, pred)\n",
    "        loss_photo = F.l1_loss(l, warped_r)\n",
    "        loss = loss_gt + alpha * loss_photo\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        s += loss.item()\n",
    "    return s / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l shape is torch.Size([1, 3, 504, 504])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of patch tokens (1295) is not a perfect square. Image size or patch size might be incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(refine_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     l \u001b[38;5;241m=\u001b[39m train_one_epoch(dino_extractor, mono_model, stereo_model, refine_net, dataloader, optimizer, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e, l)\n",
      "Cell \u001b[0;32mIn[101], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dino_extractor, mono_net, stereo_net, refine_net, dataloader, optimizer, alpha, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     fl \u001b[38;5;241m=\u001b[39m dino_extractor(l)\n\u001b[1;32m     11\u001b[0m     dm \u001b[38;5;241m=\u001b[39m mono_net\u001b[38;5;241m.\u001b[39mpredict(l)\n\u001b[1;32m     12\u001b[0m     ds \u001b[38;5;241m=\u001b[39m stereo_net\u001b[38;5;241m.\u001b[39mpredict(l, r)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch2.3.1_cu12.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch2.3.1_cu12.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 21\u001b[0m, in \u001b[0;36mDinoV2FeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(num_patches))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m*\u001b[39m s \u001b[38;5;241m!=\u001b[39m num_patches:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of patch tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_patches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not a perfect square.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Image size or patch size might be incompatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# s = int(math.sqrt(n - 1))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m p \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreshape(b, s, s, d)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of patch tokens (1295) is not a perfect square. Image size or patch size might be incompatible."
     ]
    }
   ],
   "source": [
    "\n",
    "dino_extractor = DinoV2FeatureExtractor(dino_model).cuda()\n",
    "refine_net = DisparityRefinementNet(in_channels=2, feat_channels=768, out_channels=1).cuda()\n",
    "dataset = m2014_dataset\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "optimizer = optim.AdamW(refine_net.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "for e in range(10):\n",
    "    l = train_one_epoch(dino_extractor, mono_model, stereo_model, refine_net, dataloader, optimizer, alpha=0.1, device=\"cuda\")\n",
    "    print(e, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3.1_cu12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
